{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/masa-su/pixyz/blob/main/tutorial/Japanese/00-PixyzOverview.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nagailabpbl3/.local/share/virtualenvs/multimodal_generation-otsyb3SM/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p_{prior}(z)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p_{prior}, distribution_name=Normal,\n",
      "    var=['z'], cond_var=[], input_var=[], features_shape=torch.Size([64])\n",
      "    (loc): torch.Size([1, 64])\n",
      "    (scale): torch.Size([1, 64])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle p_{prior}(z)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.distributions import Normal, Bernoulli\n",
    "from pixyz.utils import print_latex\n",
    "\n",
    "z_dim = 64\n",
    "\n",
    "#prior\n",
    "#N(z; 0, 1)\n",
    "prior = Normal(loc=torch.tensor(0.),\n",
    "               scale=torch.tensor(1.),\n",
    "               var=[\"z\"],\n",
    "               features_shape=[z_dim],\n",
    "               name=\"p_{prior}\").to(device)\n",
    "print(prior)\n",
    "print_latex(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x|z)\n",
      "Network architecture:\n",
      "  Generator(\n",
      "    name=p, distribution_name=Bernoulli,\n",
      "    var=['x'], cond_var=['z'], input_var=['z'], features_shape=torch.Size([])\n",
      "    (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
      "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (fc3): Linear(in_features=512, out_features=784, bias=True)\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle p(x|z)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dim = 784\n",
    "\n",
    "#generative model p(x|z)\n",
    "class Generator(Bernoulli):\n",
    "    def __init__(self, x_dim, z_dim):\n",
    "        super(Generator, self).__init__(var=[\"x\"], cond_var=[\"z\"], name=\"p\")\n",
    "        self.fc1 = nn.Linear(z_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, x_dim)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"probs\": torch.sigmoid(self.fc3(h))}\n",
    "\n",
    "p = Generator(x_dim, z_dim).to(device)\n",
    "print(p)\n",
    "print_latex(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  q(z|x)\n",
      "Network architecture:\n",
      "  Interface(\n",
      "    name=q, distribution_name=Normal,\n",
      "    var=['z'], cond_var=['x'], input_var=['x'], features_shape=torch.Size([])\n",
      "    (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (fc3_1): Linear(in_features=512, out_features=64, bias=True)\n",
      "    (fc3_2): Linear(in_features=512, out_features=64, bias=True)\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle q(z|x)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#interface model p(z|x)\n",
    "class Interface(Normal):\n",
    "    def __init__(self, x_dim, z_dim):\n",
    "        super(Interface, self).__init__(var=[\"z\"], cond_var=[\"x\"], name=\"q\")\n",
    "        self.fc1 = nn.Linear(x_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3_1 = nn.Linear(512, z_dim)\n",
    "        self.fc3_2 = nn.Linear(512, z_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"loc\": self.fc3_1(h), \"scale\": F.softplus(self.fc3_2(h))}\n",
    "\n",
    "q = Interface(x_dim, z_dim).to(device)\n",
    "print(q)\n",
    "print_latex(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'z': tensor([[-0.8692,  0.3627,  1.6204, -0.2100, -2.3270, -1.7125,  0.9110, -1.4538,\n",
      "         -0.3629,  1.4393,  2.2257,  0.4443,  0.2845,  1.1481,  0.8815, -0.5648,\n",
      "         -0.9252, -0.4958, -0.6896,  2.4310,  0.6378, -1.0607,  1.7778, -0.1394,\n",
      "         -0.3230,  0.9172,  1.7727,  1.2028, -0.3246,  0.1105, -0.2817,  1.6480,\n",
      "         -0.4157,  0.1428, -0.6948,  1.1176,  0.6853,  0.5598,  1.3857,  1.8068,\n",
      "          0.4076, -0.4483,  0.1025, -0.8067,  1.6540,  0.1772, -0.0590,  1.1351,\n",
      "          0.0423,  2.1853,  0.1270,  0.3917,  1.3915,  1.9126,  0.5511, -0.5458,\n",
      "         -2.0836,  1.5341, -0.6056,  0.1172,  0.4461, -0.2969, -1.5324, -0.4157]],\n",
      "       device='cuda:0')}\n",
      "dict_keys(['z'])\n",
      "torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "#サンプリング\n",
    "#z ~ p(z)\n",
    "prior_samples = prior.sample(batch_n=1)\n",
    "print(prior_samples)\n",
    "print(prior_samples.keys())\n",
    "print(prior_samples[\"z\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x,z) = p(x|z)p_{prior}(z)\n",
      "Network architecture:\n",
      "  p_{prior}(z):\n",
      "  Normal(\n",
      "    name=p_{prior}, distribution_name=Normal,\n",
      "    var=['z'], cond_var=[], input_var=[], features_shape=torch.Size([64])\n",
      "    (loc): torch.Size([1, 64])\n",
      "    (scale): torch.Size([1, 64])\n",
      "  )\n",
      "  p(x|z):\n",
      "  Generator(\n",
      "    name=p, distribution_name=Bernoulli,\n",
      "    var=['x'], cond_var=['z'], input_var=['z'], features_shape=torch.Size([])\n",
      "    (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
      "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (fc3): Linear(in_features=512, out_features=784, bias=True)\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle p(x,z) = p(x|z)p_{prior}(z)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#同時分布の定義\n",
    "#p_(x,z) = p(x|z)p(z)\n",
    "p_joint = p * prior\n",
    "print(p_joint)\n",
    "print_latex(p_joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'z': tensor([[-1.0338, -1.6246, -1.2903, -0.2704,  1.8970,  0.8257, -0.6334,  0.5154,\n",
      "          0.7881,  2.0331, -1.1795, -0.6127,  1.2353,  1.7585, -0.6222,  0.6799,\n",
      "          1.0940, -0.0681, -1.5002, -1.0008, -0.5492,  0.1305,  1.9852, -0.0512,\n",
      "          1.5764, -0.0717,  0.1816,  1.7187,  2.0722, -0.7183,  0.5219,  0.8634,\n",
      "          0.4418,  0.5128, -0.9097,  0.0071, -0.6311,  0.4591,  0.8982,  0.5703,\n",
      "         -1.2073,  0.9992,  0.4002, -0.4339,  0.4324,  0.4580,  0.8162, -0.1648,\n",
      "         -0.6515,  0.3465, -0.3718, -1.0434, -0.6909, -0.3027, -1.0341,  0.4750,\n",
      "         -1.7513, -0.5857, -0.5222, -0.1897, -0.0980, -0.8155,  1.6731,  0.3113]],\n",
      "       device='cuda:0'), 'x': tensor([[0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
      "         1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
      "         1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "         1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "         1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "         0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 1., 1.]], device='cuda:0')}\n",
      "dict_keys(['z', 'x'])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "#同時分布からサンプリング\n",
    "#x,z ~ p(x,z)\n",
    "p_joint_samples = p_joint.sample(batch_n=1)\n",
    "print(p_joint_samples)\n",
    "print(p_joint_samples.keys())\n",
    "print(p_joint_samples[\"x\"].shape)\n",
    "print(p_joint_samples[\"z\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的関数の設計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle D_{KL} \\left[q(z|x)||p_{prior}(z) \\right]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import KullbackLeibler\n",
    "\n",
    "kl = KullbackLeibler(q, prior)\n",
    "print_latex(kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconst = -p.log_prob().expectation(q)\n",
    "print_latex(reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle mean \\left(D_{KL} \\left[q(z|x)||p_{prior}(z) \\right] - \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_loss = (kl + reconst).mean()\n",
    "print_latex(vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(552.0492, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_x = torch.randn([4, 784]).to(device)\n",
    "vae_loss.eval({\"x\": dummy_x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル，訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributions (for training):\n",
      "  p(x|z), q(z|x)\n",
      "Loss function:\n",
      "  mean \\left(D_{KL} \\left[q(z|x)||p_{prior}(z) \\right] - \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)\n",
      "Optimizer:\n",
      "  AdamW (\n",
      "  Parameter Group 0\n",
      "      amsgrad: False\n",
      "      betas: (0.9, 0.999)\n",
      "      capturable: False\n",
      "      eps: 1e-08\n",
      "      foreach: None\n",
      "      lr: 0.001\n",
      "      maximize: False\n",
      "      weight_decay: 0.01\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle mean \\left(D_{KL} \\left[q(z|x)||p_{prior}(z) \\right] - \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.models import Model\n",
    "model = Model(loss=vae_loss, distributions=[p, q],\n",
    "             optimizer=optim.AdamW, optimizer_params={\"lr\": 1e-3})\n",
    "print(model)\n",
    "print_latex(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_x = torch.randn([10, 784])\n",
    "def train_dummy(epoch, x):\n",
    "    x = x.to(device)\n",
    "    loss = model.train({\"x\": x})\n",
    "    print('Epoch: {} Train Loss: {:4f}'.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: -4642.977539\n",
      "Epoch: 1 Train Loss: -4646.271973\n",
      "Epoch: 2 Train Loss: -4649.668945\n",
      "Epoch: 3 Train Loss: -4636.995117\n",
      "Epoch: 4 Train Loss: -4655.151855\n",
      "Epoch: 5 Train Loss: -4656.193848\n",
      "Epoch: 6 Train Loss: -4656.672852\n",
      "Epoch: 7 Train Loss: -4656.166992\n",
      "Epoch: 8 Train Loss: -4660.292480\n",
      "Epoch: 9 Train Loss: -4662.324219\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train_dummy(epoch, dummy_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('multimodal_generation-otsyb3SM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a69b97916829f3bbfc33d6575c6d46ede2ac1b7320149a5f3e244ba34c4468ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
